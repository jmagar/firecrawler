# ===================================
# Firecrawl Environment Configuration
# ===================================
# This is an example configuration file. Copy to .env and update values as needed.
# Sensitive values should be stored in .env.local (not tracked in git).

# ===== Core Service Configuration =====
HOST=0.0.0.0
NUM_WORKERS_PER_QUEUE=4
# Tune per CPU: e.g., 2-4 for local dev, 8-16 for production
PORT=3002

# ===== Redis Configuration =====
# For self-hosting using docker: redis://redis:6379
# For local development: redis://localhost:6379
REDIS_RATE_LIMIT_URL=redis://redis:6379
REDIS_URL=redis://redis:6379
## Production examples (separate DBs with auth):
# REDIS_URL=redis://:strongpassword@redis-jobs:6379/0
# REDIS_RATE_LIMIT_URL=redis://:strongpassword@redis-ratelimit:6379/0
# NOTE: Use separate Redis instances or DB numbers, secure credentials from secrets manager,
# and update docker-compose/deployment configs to consume these variables

# ===== Playwright Service =====
PLAYWRIGHT_MICROSERVICE_URL=http://playwright-service:3000/scrape

# ===== LLM Configuration =====
# Add your OpenAI API key for LLM-dependent features (image alt generation, etc.)
# Get your key from: https://platform.openai.com/api-keys
MODEL_NAME=gpt-4o
OPENAI_API_KEY=your-openai-api-key-here

# ===== REQUIRED VARIABLES =====

# Vector Search Configuration (REQUIRED)
# Must match your embedding model's output dimension
# Common values: 384 (MiniLM), 768, 1024, 1536 (OpenAI small), 3072 (OpenAI large)
VECTOR_DIMENSION=384

# Embedding Model (REQUIRED for vector search)
MODEL_EMBEDDING_NAME=sentence-transformers/all-MiniLM-L6-v2

# ===== Vector Search & Embeddings (TEI) =====
# Text Embeddings Inference service configuration
ENABLE_VECTOR_STORAGE=true
VECTOR_STORAGE_ASYNC=true
MIN_SIMILARITY_THRESHOLD=0.7
# Cosine similarity in [0,1]; higher = stricter match
TEI_API_KEY=
TEI_URL=http://localhost:8080
# Model for embeddings - use sentence-transformers models for TEI
MAX_EMBEDDING_CONTENT_LENGTH=200000
# Units: characters (post-HTML extraction). Large values increase RAM/CPU

# ===== PERFORMANCE CONFIGURATION =====

# Maximum threshold history size (default: 10)
# Controls memory usage in vector search
MAX_THRESHOLD_HISTORY=10

# Config file change debounce delay in ms (default: 1000)
CONFIG_DEBOUNCE_DELAY=1000

# Vector search threshold floors (optional)
VECTOR_THRESHOLD_FLOOR_1=0.55
VECTOR_THRESHOLD_FLOOR_2=0.4
VECTOR_THRESHOLD_FLOOR_3=0.3

# ===== SECURITY CONFIGURATION =====

# Base directory for configuration files (optional)
# Prevents directory traversal attacks
CONFIG_BASE_DIR=/app/config

# Enable security audit logging (default: false)
SECURITY_AUDIT_LOG=true

# ===== Authentication =====
BULL_AUTH_KEY=
# REQUIRED in production: Protects Bull queue dashboard and admin UI from unauthorized access
# Use a strong random string (32+ characters recommended)
# Generate with: openssl rand -hex 32
# NEVER commit actual keys to version control - use secrets manager in production

# ===== Fire Engine Configuration =====
# Set if using the Fire Engine closed beta
# FIRE_ENGINE_BETA_URL=

# ===== Media & Performance =====
# Block media requests to save proxy bandwidth
BLOCK_MEDIA=false

# ===== Webhook Configuration =====
# For self-hosted version webhook callbacks
# SELF_HOSTED_WEBHOOK_URL=

# ===== Logging Configuration =====
# Levels: NONE, ERROR, WARN, INFO, DEBUG, TRACE
LOGGING_LEVEL=INFO

# ===== System Monitoring =====
# System monitoring thresholds (1.0 = disabled, default: 0.8)
# Set < 1.0 to enable monitoring (e.g., 0.8 for 80% threshold)
MAX_CPU=0.8
MAX_RAM=0.8

# ===== Performance Configuration =====
# Vector search performance settings
MAX_THRESHOLD_HISTORY=10
# Configuration service debouncing
CONFIG_DEBOUNCE_DELAY=1000

# ===== SearchNG Integration =====
# Configure SearchNG service for enhanced search capabilities
SEARXNG_CATEGORIES=general
SEARXNG_ENDPOINT=
SEARXNG_ENGINES=google,bing,duckduckgo

# ===== Language Filtering =====
# Set to language code (e.g., 'en', 'es', 'fr') or 'all' to disable
DEFAULT_CRAWL_LANGUAGE=en

# ===== MCP Server Configuration =====
# Only used when running the MCP server directly (apps/firecrawler)
FIRECRAWLER_HOST=localhost
FIRECRAWLER_PORT=8000
FIRECRAWLER_TRANSPORT=streamable-http
FIRECRAWL_API_URL=http://localhost:3002

# ===== MCP Server Advanced Settings =====
# These are optional and have sensible defaults
FIRECRAWLER_LOG_LEVEL=INFO
FIRECRAWLER_LOG_TO_FILE=true
FIRECRAWLER_MAX_CONCURRENT=20
FIRECRAWLER_REQUEST_TIMEOUT=15
FIRECRAWLER_RATE_LIMIT_RPM=100
FIRECRAWLER_RATE_LIMIT_RPH=1000
FIRECRAWLER_ENABLE_RATE_LIMIT=false
FIRECRAWLER_DEFAULT_LLM_MODEL=gpt-4o
FIRECRAWLER_LLM_MAX_TOKENS=1000
FIRECRAWLER_VECTOR_SEARCH_LIMIT=10
FIRECRAWLER_VECTOR_SIMILARITY_THRESHOLD=0.7
# Cosine similarity in [0,1]; higher = stricter match

# ===== Development & Testing =====
# Only for development environments
FIRECRAWLER_DEBUG=false
FIRECRAWLER_DEV_MODE=false
FIRECRAWLER_TEST_MODE=false

# ===================================
# DEPLOYMENT NOTES
# ===================================
#
# LOCAL DEVELOPMENT:
# 1. Copy this file: cp .env.example .env
# 2. Update API keys and endpoints for your environment
# 3. For Docker Compose: docker compose up
# 4. For MCP server: python -m firecrawl_mcp.server
#
# PRODUCTION:
# 1. Use .env.local for sensitive values (not tracked in git)
# 2. Set appropriate rate limits and timeouts
# 3. Enable authentication where needed
# 4. Configure proper logging levels
# 5. Set up monitoring and alerting
#
# SECURITY:
# - Never commit real API keys to version control
# - Rotate keys regularly
# - Use environment-specific configurations
# - Enable rate limiting in production

